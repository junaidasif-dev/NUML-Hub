{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7e72b7",
   "metadata": {},
   "source": [
    "# **Task 1: Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c198f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'this', 'product', '!', 'It', \"'s\", 'amazing', 'and', 'works', 'great', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love this product! It's amazing and works great.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593a47e",
   "metadata": {},
   "source": [
    "# **Task 2: Morphemes in the Word**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d515348",
   "metadata": {},
   "source": [
    "# Explain in pdf document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8028052",
   "metadata": {},
   "source": [
    "# **Task 3: Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33719ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'runner', 'run']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = [\"running\", \"runner\", \"runs\"]\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2314e22",
   "metadata": {},
   "source": [
    "# **Task 4: Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c1b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['better', 'running', 'mouse']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"better\", \"running\", \"mice\"]\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b09fd6",
   "metadata": {},
   "source": [
    "# **Task 5: Overstemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34941cf6",
   "metadata": {},
   "source": [
    "# Explain in pdf document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d90b1",
   "metadata": {},
   "source": [
    "# **Task 6: Part-of-Speech Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9545b28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ'), ('peacefully', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The cat sleeps peacefully.\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1049dc",
   "metadata": {},
   "source": [
    "# **Task 7: Filter Out Non-Alphabetic Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7daaa410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello user How are you happy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello @user! How are you? 123 #happy\"\n",
    "cleaned = \" \".join(re.findall(r'\\b[a-zA-Z]+\\b', text))\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4816720",
   "metadata": {},
   "source": [
    "# **Task 8: Check for Profanity/Offensive Language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3e8552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains Profanity: True\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is a great day, damn it!\"\n",
    "profanities = {\"damn\", \"hell\", \"shit\", \"crap\"}\n",
    "words = word_tokenize(text.lower())\n",
    "contains_profanity = any(word in profanities for word in words)\n",
    "print(\"Contains Profanity:\", contains_profanity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ce126",
   "metadata": {},
   "source": [
    "# **Task 9: One Hot Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "357953b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bird  cat  dog\n",
      "0   0.0  1.0  0.0\n",
      "1   0.0  0.0  1.0\n",
      "2   1.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "labels = [[\"cat\"], [\"dog\"], [\"bird\"]]\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded = encoder.fit_transform(labels)\n",
    "df = pd.DataFrame(encoded, columns=encoder.categories_[0])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e30b5",
   "metadata": {},
   "source": [
    "# **Task 10: Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef5c22d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acting  bad  good  movie  plot\n",
      "0       0    0     1      1     0\n",
      "1       1    1     0      0     0\n",
      "2       0    0     1      0     1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"good movie\", \"bad acting\", \"good plot\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058bd98",
   "metadata": {},
   "source": [
    "# **Task 11: TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96327d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and       cat       dog       the\n",
      "0  0.000000  0.789807  0.000000  0.613356\n",
      "1  0.000000  0.000000  0.789807  0.613356\n",
      "2  0.631745  0.480458  0.480458  0.373119\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"the cat\", \"the dog\", \"the cat and dog\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d7b52",
   "metadata": {},
   "source": [
    "# **Task 12: N-Grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbc2316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('I', 'love'), ('love', 'to'), ('to', 'code')]\n",
      "Trigrams: [('I', 'love', 'to'), ('love', 'to', 'code')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love to code\"\n",
    "tokens = word_tokenize(text)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
